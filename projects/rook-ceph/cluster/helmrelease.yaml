---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: flux-system
spec:
  chart:
    spec:
      chart: rook-ceph-cluster
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: rook-release
  interval: 1h0m0s
  values:
    clusterName: rook-ceph-cluster
    toolbox:
      enabled: true
      containerSecurityContext:
        runAsNonRoot: false
        runAsUser: 0
        runAsGroup: 0
    monitoring:
      enabled: true
      createPrometheusRules: true
    cephClusterSpec:
      # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
      dataDirHostPath: /var/lib/rook
      # TODO experiement with these to see if performance improves without issues
      network:
        encryption:
          enabled: "false"
        compression:
          enabled: "false"
      storage:
        # config:
        #   osdsPerDevice: 3
        nodes:
          - name: bigrig
            devices:
              - name: "/dev/sdb"
              - name: "/dev/sdc"
          - name: oppenheimer
            devices:
              - name: "/dev/sdc"
    cephFileSystemVolumeSnapshotClass:
      enabled: true
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
    cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
      storageClass:
        name: ceph-block
        isDefault: false
        enabled: true
        parameters:
          archiveOnDelete: "true"
    cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - failureDomain: host
            replicated:
              size: 3
            # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/v1.14.9/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
            name: data0
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            limits:
              memory: "4Gi"
            requests:
              cpu: "1000m"
              memory: "4Gi"
          priorityClassName: system-cluster-critical
      storageClass:
        name: cephfs
        isDefault: false
        enabled: true
        parameters:
          archiveOnDelete: "true"
    cephObjectStores:
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
        preservePoolsOnDelete: true
        gateway:
          port: 80
          resources:
            limits:
              memory: "2Gi"
            requests:
              cpu: "1000m"
              memory: "1Gi"
          # securePort: 443
          # sslCertificateRef:
          instances: 1
          priorityClassName: system-cluster-critical
      storageClass:
        name: ceph-object
        isDefault: false
        enabled: true
        parameters:
          archiveOnDelete: "true"