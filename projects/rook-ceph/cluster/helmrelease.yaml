---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: flux-system
spec:
  chart:
    spec:
      chart: rook-ceph-cluster
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: rook-release
  interval: 1h0m0s
  values:
    clusterName: rook-ceph-cluster
    toolbox:
      enabled: true
      containerSecurityContext:
        runAsNonRoot: false
        runAsUser: 0
        runAsGroup: 0
    monitoring:
      enabled: false
      createPrometheusRules: false
    cephClusterSpec:
      # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
      dataDirHostPath: /var/lib/rook
      # TODO experiement with these to see if performance improves without issues
      network:
        encryption:
          enabled: "false"
        compression:
          enabled: "false"
      removeOSDsIfOutAndSafeToRemove: true
      storage:
        # config:
        #   osdsPerDevice: 3
        nodes:
          - name: bigrig
            devices:
              - name: "/dev/sdb"
              - name: "/dev/sdc"
          - name: oppenheimer
            devices:
              - name: "/dev/loop3"
          - name: tiffrig
            devices:
              - name: "/dev/sda"
      resources: 
        mgr:
          limits:
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "512Mi"
        mon:
          limits:
            memory: "2Gi"
          requests:
            cpu: "1000m"
            memory: "1Gi"
        osd:
          limits:
            memory: "4Gi"
          requests:
            cpu: "1000m"
            memory: "4Gi"
        prepareosd:
          # limits: It is not recommended to set limits on the OSD prepare job
          #         since it's a one-time burst for memory that must be allowed to
          #         complete without an OOM kill.  Note however that if a k8s
          #         limitRange guardrail is defined external to Rook, the lack of
          #         a limit here may result in a sync failure, in which case a
          #         limit should be added.  1200Mi may suffice for up to 15Ti
          #         OSDs ; for larger devices 2Gi may be required.
          #         cf. https://github.com/rook/rook/pull/11103
          requests:
            cpu: "500m"
            memory: "50Mi"
        mgr-sidecar:
          limits:
            memory: "500Mi"
          requests:
            cpu: "100m"
            memory: "40Mi"
        crashcollector:
          limits:
            memory: "60Mi"
          requests:
            cpu: "100m"
            memory: "60Mi"
        logcollector:
          limits:
            memory: "1Gi"
          requests:
            cpu: "100m"
            memory: "100Mi"
        cleanup:
          limits:
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "100Mi"
        exporter:
          limits:
            memory: "128Mi"
          requests:
            cpu: "50m"
            memory: "50Mi"

    cephFileSystemVolumeSnapshotClass:
      enabled: true
    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
    cephBlockPools:
    - name: ceph-blockpool
      spec:
        failureDomain: host
        replicated:
          size: 3
      storageClass:
        name: ceph-block
        isDefault: true
        enabled: true
        parameters:
          archiveOnDelete: "true"
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephFileSystems:
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - failureDomain: host
            replicated:
              size: 3
            # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/v1.14.9/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
            name: data0
        metadataServer:
          activeCount: 1
          activeStandby: true
          resources:
            limits:
              memory: "4Gi"
            requests:
              cpu: "1000m"
              memory: "4Gi"
          priorityClassName: system-cluster-critical
      storageClass:
        name: cephfs
        isDefault: false
        enabled: true
        parameters:
          archiveOnDelete: "true"
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
          csi.storage.k8s.io/fstype: ext4
    cephObjectStores:
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
        preservePoolsOnDelete: true
        gateway:
          port: 80
          resources:
            limits:
              memory: "2Gi"
            # requests:
            #   cpu: "1000m"
            #   memory: "1Gi"
          # securePort: 443
          # sslCertificateRef:
          instances: 1
          priorityClassName: system-cluster-critical
      storageClass:
        name: ceph-object
        isDefault: false
        enabled: true
        parameters:
          archiveOnDelete: "true"
          region: us-east-1
