version: "3.9"
services:
  localai:
    container_name: localai
    tty: true
    stdin_open: true
    ports:
      - 44344:8080
    volumes:
      - ./localai.env/models:/models:cached
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
    image: localai/localai:v2.14.0-cublas-cuda12-core
    command: tinyllama-chat
